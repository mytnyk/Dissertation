
\section{Структурованість моделі. Розклад аналізу варіацій}

На практиці дослідник, як правило, рідко знаходить правильну
структуру моделі з першого разу і він вимушений повторювати спроби
знову і знову поки не отримає задовільну модель. Цей процес
відображено на рис.~\ref{fig:CiclicModelling}.
\begin{figure}[ht]
\safepdf{\centerline{\includegraphics[height=7cm]{CHAPTER1/EPS/sd_cm.pdf}}}
{\centerline{\psfig{figure=CHAPTER1/EPS/sd_cm.eps,height=7cm}}}
\caption{Циклічна структура побудови моделі.}
\label{fig:CiclicModelling}
\end{figure}
При цьому основні складові циклу побудови моделі мають наступний
зміст~\cite{Harris-Hong-Gan}:
\begin{list}{\checkmark}{}%
\item Дизайн --- вибір представлення моделі. \item Тренування
--- оцінка параметрів моделі. \item Валідація --- тестування
моделі для визначення її адекватності реальному процесу. \item
Інтерпретація --- яким чином модель пояснює (розуміє) реальний
процес?
\end{list}
Зазначимо, що структурованість моделі, тобто її властивість до
легкої інтерпретації, відіграє неабияку роль в правильному уточненні
структури моделі. Тому в роботі приділяється увага розробці саме
структурованих, з точки зору інтерпретації, моделей. Формальне
визначення класу структурованих моделей наведено в
роботі~\cite{Zajchenko-IS}. Відповідно до нього структурована модель
розглядається як сукупність операторів, суперпозиція яких здійснює
відображення вхідних факторів (функцій) у вихідний фактор (функцію).

Пошук структури моделі у вигляді розкладу аналізу варіацій:
$$
\eta(X)=\sum_j\eta_j(X_j)+\sum_{j>k}\eta_{jk}(X_j,X_k)+\sum_{j>k>l}\eta_{jkl}(X_j,X_k,X_l)+\ldots,
$$
де вивчається залежність фактору $X$ від від факторів $X_j$, є
найбільш поширеним способом забезпечення структурованості. Так,
наприклад, робота~\cite{VoznesenskyKovalchuk78} присвячена пошуку і
прийняттю рішення по поліноміальній статистичній $n$-факторній
моделі другого порядку:
$$
\eta(\mathbf{x})=\beta_0+\sum_{j=1}^n\beta_jx_j+\sum_{j=1}^n\beta_{jj}x_j^2+\sum_{i<j}\beta_{ij}x_ix_j.
$$
В цьому підрозділі розглядається декілька типових застосувань такого
розкладу, якими є представлення моделі у вигляді ряду Вольтера і
його дискретного аналогу --- узагальненого поліному
Колмогорова-Габора.

\subsection{Описання динамічних процесів за допомогою ряду
Вольтера}

Математичний апарат рядів Вольтера добре вивчений~\cite{Volterra,
Viner, Li-Shchecen}. Також можна знайти багато джерел по методах
моделювання на основі функціональних рядів Вольтера
(ФРВ)~\cite{Shchecen, Maas}, тобто методах, які дозволяють відшукати
ядра Вольтера. В.М.Пінчук~\cite{Pinchuk} розробив метод
ідентифікації динамічних стаціонарних об'єктів керування, який
полягає в ітеративній побудові ФРВ. Інший інтегральний метод
ідентифікації динамічних об'єктів за експериментальними даними
запропоновано в~\cite{AmidGazi}. В роботі~\cite{Prazenica-Kurdila}
запропоновано метод ідентифікації ядер Вольтера другого порядку
(двох змінних) за допомогою вейвлетів з трикутним носієм.

Однією з сучасних розробок на основі рядів Вольтера є метод навчання
нейронних мереж А.І.Іванова~\cite{Ivanov, Ivanov-diss}, в якому
кожному ряду Вольтера ставиться у відповідність нейронна мережа і,
як наслідок, всі розробки для рядів Вольтера переносяться на теорію
побудови та навчання нейронних мереж. Так, зокрема, симетризація
ядер Вольтера дозволяє знизити складність задачі навчання нейронних
мереж з експонентної до поліноміальної (лінійної або квадратичної).
Основною ідеєю є використання рядів Вольтера для ідентифікації
структури економічної моделі як системи з нескінченною пам'яттю.
Саме такі системи пояснюють нелінійні динамічні процеси в економіці:
явища гістерезису та наявність ділових циклів.


\subsubsection{Ідентифікація ФРВ за допомогою розкладу ядер за
системою ортогональних поліномів}

В роботі~\cite{Krug-Sosulin} розвинена техніка аналізу ФРВ з
допомогою розкладу ядер за системою базисних функцій. Для вхідного
сигналу будується спеціальна система функцій, які є лінійними
згортками входу і вибраного базису функцій, далі вихід системи
розкладається в ряд по цим функціям. Гарні апроксимаційні
властивості для широкого класу систем мають наближення вигляду:
\begin{equation}
\label{2Volterra} \hat{y}(t)=h_0+\int h_1(\tau)x(t-\tau)d\tau+\iint
h_2(\tau_1, \tau_2)x(t-\tau_1)x(t-\tau_2)d\tau_1d\tau_2.
\end{equation}
Це відповідає представленню першими членами функціонального ряду
Вольтера. Припустимо, що ядра $h_1$, $h_2$ допускають розклад по
деякій системі базисних функцій $\{\varphi_i(\tau, \alpha) :
i=1,\ldots,k\}$:
\begin{equation}
\label{Volterra-basis} h_1(\tau)=\sum_{i=1}^kw_i\varphi_i(\tau,
\alpha),\quad h_2(\tau_1,
\tau_2)=\sum_{i,j=1}^kw_{ij}\varphi_i(\tau_1,
\alpha)\varphi_j(\tau_2, \alpha).
\end{equation}
Тоді вираз~(\ref{2Volterra}), враховуючи~(\ref{Volterra-basis}),
приймає вигляд:
\begin{equation}
\label{Volterra-regression}
\hat{y}(t)=w_0+\sum_{i=1}^kw_if_i(t)+\sum_{i,j=1}^kw_{ij}f_i(t)f_j(t),
\end{equation}
де $f_i(t)=\int\varphi_i(\tau,
\alpha)x(t-\tau)d\tau$ --- реакція $i$-ого фільтра на вхідний
сигнал $x(t)$.

В якості системи базисних функцій можна, наприклад,
використовувати систему функцій Лагера, що дозволяє найкращим
чином апроксимувати імпульсні перехідні характеристики
аперіодичних і слабоколивальних об'єк\-тів. Система функцій Лагера
має вигляд:
$$
\varphi_i(\tau,
\alpha)=\sqrt{2\alpha}e^{-\alpha\tau}\sum_{j=0}^{i-1}\frac{(i-1)!(-2\alpha)^j}{(i-j-1)!(j!)^2}\tau^j.
$$

Зважаючи на специфіку того чи іншого об'єкта, систему базисних
функцій можна формувати з міркувань найкращого наближення ядер і
використовувати для цього спеціальні класи поліномів, зокрема
ортогональні поліноми Чебишева 2-го роду. Переваги цих поліномів
описані в~\cite{Lancosh}.

Рівняння регресії ~(\ref{Volterra-regression}) також можна
узагальнити на випадок системи з багатьма вхідними змінними, якщо
представити всі виходи фільтрів для всіх вхідних змінних, як
виходи одного узагальненого набору фільтрів:
$$
\{f_i(x_r(t)): i=1,\ldots,k; r=1,\ldots,n\}\longmapsto \{f_p(t):
p=1,\ldots,kn\}.
$$
Тоді узагальнений вихід запишеться у формі:
$$
\hat{y}(t)=w_0+\sum_{p=1}^{kn}w_pf_p(t)+\sum_{p,q=1}^{kn}w_{pq}f_p(t)f_q(t).
$$

\subsection{Метод групового врахування аргументів}

МГВА запропонований в роботах
О.Г.Івахненка~\cite{Ivahkenko-70,Ivahnenko,Ivahkenko-Stepashko}
використовує ідеї самоорганізації та механізми еволюції такі як
схрещування аргументів і генерація нащадків, селекція та відбір
кращих. Основними перевагами методу є~\cite{Zajchenko-Zaetc}:
\begin{list}{$\bullet$}{}
\item Метод не потребує задавання моделі у явному вигляді, модель
будується автоматично в процесі роботи алгоритму. \item Метод працює
на коротких вибірках, розмір яких незначно більший за число
коефіцієнтів моделі.
\end{list}
При цьому класичний МГВА має досить суттєві недоліки:
\begin{list}{$\bullet$}{}
\item При близьких експериментальних точках ймовірна поява
виродженості матриці нормальних рівнянь Гауса, так званий
``індуцит'', внаслідок чого виникає необхідність застосування
методів регуляризації. \item Метод видає точкову модель прогнозу,
хоч іноді бажано мати довірчий інтервал, яких характеризує точність
прогнозу.
\end{list}

Повна залежність між входом та виходом шукається в класі
поліноміальних моделей де може бути представлена у вигляді
узагальненого поліному Колмогорова-Габора:
\begin{equation}
\label{GMDH-Kolmogorov-exp}
\mathcal{M}(\mathbf{x},\mathbf{a})=a_0+\sum_{i=1}^na_ix_i+\sum_{j=1}^n\sum_{i\leqslant
j}a_{ij}x_ix_j+\ldots
\end{equation}
При побудові моделі і визначенні коефіцієнтів в якості критерію
оптимальності використовується наступний критерій:
$$
L(f)=\frac{1}{N}\sum_{t=1}^N\left(y_t-f(\mathbf{x}_t)\right)^2.
$$
Тобто мінімізується функціонал $L(f)$, який співпадає з емпіричним
ризиком для фіксованої структури моделі~(\ref{EmpiricalRisk}). Для
зменшення структурного ризику і подолання неповноти вибірки даних,
яка є наслідком теореми неповноти Геделя, використовується принцип
зовнішнього доповнення. В якості зовнішнього доповнення виступає
додаткова перевірочна вибірка, точки якої не використовувалися при
навчанні системи, тобто при пошуку оціночних коефіцієнтів
поліному~(\ref{GMDH-Kolmogorov-exp}). Це відповідає методу
перехресної валідації з підрозділу~\ref{sec-cross_valid} Таким чином
пошук найкращої моделі здійснюється наступним чином:
\begin{enumerate}
\item Вся наявна вибірка даних $\mathcal{D}$ ділиться на навчальну
$\mathcal{D}_{N_{\mbox{\small навч}}}$ та перевірочну
$\mathcal{D}_{N_{\mbox{\small перев}}}$:
$N=N_{\mbox{навч}}+N_{\mbox{перев}}$.
\item На навчальній вибірці $\mathcal{D}_{N_{\mbox{\small навч}}}$ визначається вектор
параметрів $\mathbf{a}$. \item На перевірочній вибірці
$\mathcal{D}_{N_{\mbox{\small перев}}}$ відбираються кращі моделі.
\end{enumerate}
Складність моделі $S$ визначається розмірністю вектору параметрів
$\mathbf{a}$, тобто кількістю коефіцієнтів поліному. Зазначимо, що
складність $S$ є аналогом розмірності Вапника-Червоненкіса.

Схема класичного МГВА включає:
\begin{enumerate}
\item Для кожної пари $x_i$, $x_j$ будуються часткові описи або
субмоделі (всього $C_n^2$) вигляду:
\begin{list}{---}{} \item або лінійні
$$
\hat{y}^{(s)}=\phi(x_i,x_j)=a_0+a_ix_i+a_jx_j,\quad
s=1,\ldots,C_n^2.
$$
\item або квадратичні
$$
\hat{y}^{(s)}=\phi(x_i,x_j)=a_0+a_ix_i+a_jx_j+a_{ii}x_i^2+a_{ij}x_ix_j+a_{jj}x_j^2.
$$
\end{list}
\item Визначаємо коефіцієнти цих моделей, тобто вектор параметрів
$\mathbf{a}$, за допомогою МНК, використовуючи навчальну вибірку.
\item На перевірочній вибірці для кожної з побудованих моделей
шукаємо оцінку:
\begin{equation}\label{RiskCrossValid}
L_s=\frac{1}{N_{\mbox{перев}}}\sum_{t=1}^{N_{\mbox{\small
перев}}}\left(y_t-\hat{y}^{(s)}_t\right)^2,
\end{equation}
де $y_t$ --- дійсне значення виходу системи в момент $t$,
$\hat{y}^{(s)}_t$ --- значення виходу моделі $s$ в момент $t$ на
перевірочній вибірці. Визначаємо $F_1$ кращих моделей з найменшим
значенням функціоналу $L_s$. Відібрані моделі подаються на другий
ряд:
$$
\phi^{(2)}(y_i,y_j)=a^{(2)}_0+a^{(2)}_iy_i+a^{(2)}_jy_j+a^{(2)}_{ii}y_i^2+
a^{(2)}_{ij}y_iy_j+a^{(2)}_{jj}y_j^2.
$$
Оцінка моделей застосовується така ж як і на першому ряді, але
відбирається $F_2<F_1$ кращих моделей.
\end{enumerate}
Процес побудови рядів повторюється поки значення функціоналу, який
має зміст середнього квадрату помилки, зменшується. Якщо в ряду $m$
отримаємо збільшення помилки, то процес завершується.

\subsubsection{Нечіткий метод групового врахування аргументів}

В роботі~\cite{Zajchenko-Zaetc,Zajchenko-IS} була розглянута лінійна
інтервальна модель регресії:
\begin{equation} \label{LinIntervalModel}
Y=A_0z_0+A_1z_1+\ldots+A_pz_p\quad,
\end{equation}
де $A_i$ --- нечіткі числа трикутного вигляду, які задаються двома
параметрами:
$$
A_i=\left(a_i,c_i\right),
$$
де $a_i$ --- центр інтервалу, $c_i$ --- ширина інтервалу,
$c_i\geqslant 0$. Відповідно $Y$ --- нечітке число, параметри якого
(центр і ширина) визначаються наступним чином:
$$
a_Y=\sum_{i=1}^pa_iz_i=\mathbf{a}^\top \mathbf{z},\quad
c_Y=\sum_{i=1}^pc_i|z_i|=\mathbf{c}^\top |\mathbf{z}|.
$$
Інтервальна модель є коректною, якщо дійсне значення вихідної
величини $Y$ належить інтервалу невизначеності:
$$
\left\{\begin{array}{c}\mathbf{a}^\top \mathbf{z}-\mathbf{c}^\top |\mathbf{z}|\leqslant y\\
\mathbf{a}^\top \mathbf{z}+\mathbf{c}^\top |\mathbf{z}|\geqslant y
\end{array}\right..
$$
Якщо розглянути вибірку
$\mathcal{D}=\{(y_t,\mathbf{z}_t):t=1,\ldots,N\}$, тоді
модель~(\ref{LinIntervalModel}) є коректною, якщо множина
$\{(a_i,c_i):i=1,\ldots,p\}$ задовольняє обмеженням:
$$
\left\{\begin{array}{c}\mathbf{a}^\top \mathbf{z}_t-\mathbf{c}^\top |\mathbf{z}_t|\leqslant y_t\\
\mathbf{a}^\top \mathbf{z}_t+\mathbf{c}^\top |\mathbf{z}_t|\geqslant
y_t
\end{array}\right.,\quad t=1,\ldots,N.
$$
Таки чином для оціночної лінійної інтервальної моделі необхідно
знайти такі значення параметрів $(a_i,c_i)$ нечітких коефіцієнтів
$A_i$, які задовольняють:
\begin{list}{$\bullet$}{}
\item Дійсні значення виходу $y_t$ належать оціночному інтервалу
для $Y_t$. \item Сумарна ширина оціночного інтервалу мінімальна.
\end{list}
Ці вимоги для квадратичного часткового опису можна звести до
наступної задачі лінійного програмування:
\begin{multline}\label{min_LP_task_GMDH}
c_0N+c_1\sum_{t=1}^N|x_{it}|+c_2\sum_{t=1}^N|x_{jt}|+c_3\sum_{t=1}^N|x_{it}\cdot
x_{jt}|+\\
+c_4\sum_{t=1}^N|x^2_{it}|+c_5\sum_{t=1}^N|x^2_{jt}|\longrightarrow
\min,
\end{multline}
з обмеженнями:
\begin{equation}\label{restriction_LP_task_GMDH}
\left\{\begin{array}{c}a_0+a_1x_{it}+a_2x_{jt}+a_3(x_{it}\cdot
x_{jt})+a_4x^2_{it}+a_5x^2_{jt}-c_0-\\-c_1|x_{it}|-c_2|x_{jt}|-c_3|x_{it}\cdot
x_{jt}|-c_4|x^2_{it}|-c_5|x^2_{jt}|\leqslant
y_t\\a_0+a_1x_{it}+a_2x_{jt}+a_3(x_{it}\cdot
x_{jt})+a_4x^2_{it}+a_5x^2_{jt}+c_0+\\+c_1|x_{it}|+c_2|x_{jt}|+c_3|x_{it}\cdot
x_{jt}|+c_4|x^2_{it}|+c_5|x^2_{jt}|\geqslant
y_t\\
c_0,c_1,c_2,c_4,c_5\geqslant0, \quad t=1,\ldots,N\end{array}\right.
\end{equation}
Але задача у формі
(\ref{min_LP_task_GMDH}-\ref{restriction_LP_task_GMDH}) не є зручною
для застосування стандартних алгоритмів, оскільки відсутня умова
невід'ємності коефіцієнтів $a_l$. Таким чином для її вирішення
необхідно перейти до двоїстої задачі, яка розв'язується
симплекс-методом.

\subsubsection{Алгоритм НМГВА}

\begin{algorithmic}
   \STATE {\bfseries Задані:} дані спостережень $\mathcal{D}$, загальний вигляд моделі і опорних функцій;
   зовнішній критерій оптимальності (функціонал $L$), рівень збіжності $\epsilon>0$.
   \STATE {\bfseries Результат:} оптимальна модель $\mathcal{M}_{\mbox{опт}}$.
   \STATE {\bfseries Алгоритм починає роботу з}
   \STATE розбиття загальної вибірки на навчальну $\mathcal{D}_{N_{\mbox{\small навч}}}$
    та перевірочну $\mathcal{D}_{N_{\mbox{\small перев}}}$;%
   \STATE ітератор числа рядів $r\longleftarrow0$;%
   \STATE кількість входів $F^{(0)}\longleftarrow n$;%
   \REPEAT
   \STATE генерація множини часткових моделей кандидатів на основі вибраної опорної функції:
   $\left\{\mathcal{M}_i^{(r)}:i=1,\ldots,\dbinom{2}{F^{(r)}}\right\}$;\\
   \FOR{{\bfseries кожної} часткової моделі кандидата $\mathcal{M}_i^{(r)}$}
   \STATE розв'язуємо задачу~(\ref{min_LP_task_GMDH}-\ref{restriction_LP_task_GMDH}) і
        знаходимо значення $(a_i,c_i)$;
   \STATE обчислюємо значення зовнішнього критерію $L_i^{(r)}=L\left(\mathcal{M}_i^{(r)}\right)$;%
   \ENDFOR перебору часткових моделей кандидатів;
   \STATE обчислення середнього критерію $L^{(r)}$ для часткових моделей;
   \STATE $r\longleftarrow r+1$;
   \STATE вибір $F$ кращих моделей і формування входів нового ряду $F^{(r)}\longleftarrow F$;%
   \UNTIL{не виконається критерій зупинки $|L^{(r+1)}-L^{(r)}|<\epsilon$};%
   \STATE з $F$ кращих моделей передостаннього ряду вибирається оптимальна модель $\mathcal{M}_{\mbox{опт}}$
   і відновлюється її аналітичний вигляд;
   \STATE {\bfseries кінець роботи алгоритму.}
\end{algorithmic}
