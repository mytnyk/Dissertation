
%Зробити тут треба повний огляд
\subsection{Розвиток адаптивного моделювання і керування}

Вивчення структури головного мозку і механізмів його роботи
призвело до розвитку нових обчислювальних методів, які засновані
на біохімічних поняттях і придатні для вирішення таких складних
задач, як розпізнавання образів, моделювання, швидке перетворення
інформації (паралельні обчислювальні процеси), ідентифікація і
управління. Цими новими обчислювальними моделями стали штучні
нейронні мережі (ШНМ) та нечітка логіка.

Ще на початку XX сторіччя Павлов відкрив елементарний
функціональний блок мозку --- умовний рефлекс. Він вважав умовний
рефлекс одним з основних ``атомів'' інтелектуальних процесів, що
відбуваються в корі головного мозку. Ним і іншими вченими були
висунуті свого часу різні гіпотези механізмів адаптивної
центральної нервової системи.

Перший крок зробили у 1943 році нейрофізіолог Маккалок і математик
Піттс. У своїй спільній статті вони вперше описали модель штучного
нейрона у вигляді електричної схеми і ввели поняття штучної
нейронної мережі. Пізніше вони провели дослідження парадигм
побудови нейронних мереж, у вузлах яких помістили штучні нейрони.
Першим практичним застосуванням таких мереж було розпізнавання
зображень, які зсувались або повертались.

Ще далі пішов Уідроу, який навів переконливі докази нових на той
час можливостей штучного нейрона. Модель штучного нейрона мала
надзвичайно простий вигляд і відображала лише основні функції
нейрона біологічного. На вхід моделі подавалися сигнали $X_1$,
$X_2,\ldots,X_n$, які відповідно множилися на відповідні
коефіцієнти ваги $W_1$, $W_2,\dots,W_n$, далі добутки надходили до
суматору $S$. Сумарний сигнал подавався на функціональний
перетворювач $F$, на виході якого отримували вихідний сигнал $Y$.
У найпростішому випадку у ролі перетворювача $F$ виступав
пороговий елемент $T$ --- якщо сума була більша порогового
значення, вихід нейрона давав $1$, якщо меншою, то $0$.

Після того, як американський дослідник Хебб сформулював відкритий
ним принцип навчання  штучного нейрона (правило Хебба), з’явилась
реальна можливість побудови працюючої штучної мережі. І невдовзі
вона дійсно була створена --- в 1957 р. нейробіолог Розенблат,
працюючи в університеті Корнеля, США, вперше математично описав
найпростішу нейронну структуру --- перцептрон (від лат. {\em
реrceptio} --- сприйняття) і показав його здатність до навчання.
Розенблат визначив перцептрон як множину елементів (``нейронів''),
які генерують сигнали і зв'язані у єдину мережу. Логічні
властивості перцептрона визначаються:
\begin{itemize}
\item його топологічною структурою, тобто зв'язками між
``нейронами''; \item набором алгоритмів, що управляють генерацією
і передачею сигналів; \item набором функцій пам'яті або алгоритмів
перетворення властивостей мережі в результаті активності.
\end{itemize}

Подібний автомат для розпізнавання зображень у 1959 році, через 2
роки після Розенблата і незалежно від нього сконструював
український вчений академік О.Г. Івахненко. Його нейроавтомат
“Альфа” поклав початок дослідженням нейрокомп'ютингу в Європі.
Незабаром з'явився метод групового врахування аргументів.

Перцептрон Розенблата складався з одного шару штучних нейронів,
зв'язки між якими виконували функції елементів пам'яті і могли
підстроюватися під різні комбінації вхідних сигналів. У порівнянні
із штучним нейроном перцептрон мав розширені можливості, в тому
числі і кращу здатність до навчання, оскільки тут кожен елемент
вхідного сигналу $X_i$ поступав на кожен вхід $n$ з відповідним
ваговим коефіцієнтом $W_n$ - в процесі навчання усі вагові
коефіцієнти поступово видозмінювалися таким чином, щоб вихідний
сигнал y був якомога ближчим до зразкового сигналу $Y_0$. Цю
визначну на той час здатність перцептрона навчатися на зразках
Розенблат сформулював у вигляді теореми, яку довів теоретично і
підтвердив практично. Він продемонстрував роботу перцептрона на
прикладі класифікації вхідних сигналів на два класи, апаратно
реалізувавши його модель на електронних порогових елементах.

Модель перцептрона Розенблата з невеликими доповненнями
використовується і сьогодні, в основному з навчальною метою як
модель найпростішої штучної нейромережі. Доповнення стосуються в
основному двох моментів. По-перше, перетворювач $F$ задається у
вигляді нелінійних функцій. По-друге, вводиться елемент зміщення
сигналу суматору, що дозволяє динамічно задавати робочу точку на
нелінійній характеристиці перетворювача $F$. Ці доповнення значно
розширюють функціональні можливості перцептрона і, що
найголовніше, полегшують процес його навчання.

У 1960 році Уідроу і Хофф розробили модель нейрона, яка навчалася
швидше і більш точно, ніж перцептрон. Вона отримала назву АдаЛіНе
(адаптивний лінійний нейрон), як продовження АдаЛінЕ (адаптивного
лінійного елементу). Алгоритм настройки ґрунтувався на методі
найменших квадратів. Цим алгоритмом вперше була введена концепція
навчання ``з вчителем''.

в 1963 році Мінскі і Пейперт опублікували книгу, в якій
аналізувалася проста лінійна мережа MAdaLiNe (Many AdaLiNe). В ній
був встановлений початок строгому аналізу перцептронних схем. Але
песимістичні висновки, отримані в результаті цих досліджень,
зупинили процес вивчення проблеми майже на 20 років. Виявилося, що
однорівневий перцептрон дуже обмежений у своїх можливостях --– в
окремих випадках він не може реалізувати навіть найпростіші
функції. Найбільший песимізм відносно перцептрона Розенблата
викликало те, що він принципово не міг розділити дві області, які
описувались елементарною логічною функцією ЗАПЕРЕЧЕННЯ АБО двох
Булевих змінних $X$ і $Y$. Таким чином стверджувалося, що
однорівневі лінійні мережі мають обмежені можливості (що у
принципі вірно), а навчання багаторівневих мереж є непродуктивним.
Авторитет Мінскі і Пейперта був настільки великий, що їх висновок
став своєрідним вироком, і інтерес до проблеми штучних нейронних
мереж був втрачений на багато років, та й сама ідея створення
штучного інтелекту на їх основі надовго відійшли у тінь.

Одним із значних досягнень в 70-і роки став CMAC (cerebellum model
articulation controller) розвинений Альбусом. Це була перша модель
нейромережі з асоціативною пам'яттю, яка була лінійною в
параметрах. CMAC фактично намагався повторити функціональність
мозку.

В 1982 р. з'явилася робота Хопфілда, де він описав спеціальну
динамічну структуру, розроблену їм для вирішення оптимізаційних
задач, яка одержала назва мережі Хопфілда. Хопфілд представив
доповідь в Національну Академію США, де показав можливість
побудови нової архітектури нейромережі на базі все того ж
перцептрона, але з від’ємним зворотнім зв'язком між нейронами. Для
її навчання були розроблені різні методи і алгоритми. В 1986 році
дослідницька група по паралельним обчисленням на чолі з
Румельхартом розробила обчислювальну процедуру навчання
багаторівневих нейронних мереж (БНМ), що одержала назву ``error
back propagation method'' --- метод зворотного розповсюдження
помилки. Модель Хопфілда виявилась настільки плідною, що у своєму
первісному вигляді використовується і сьогодні для вирішення
окремих практичних задач. З часом з’явились нові парадигми штучних
нейронних мереж, значно складніші за своєю будовою, однак
перцептрон Розенблата і нейромережа Хопфілда залишаються як
класичні зразки перших нейрокомп'ютерів, які сьогодні відносять до
6-го покоління еволюції ЕОМ.

У розвиток теорії штучного інтелекту значний внесок внесли
радянські вчені. Ще в 1966 році Червоненкіс і Вапник розробили
основи так званої ``теорії розмірності'', що дозволила дати оцінку
можливостей систем, що навчаються, взагалі і нейронних мереж,
зокрема. Фундаментальні дослідження в 60-і роки в області
перцептронних систем були виконані Глушковим і Івахненко. У
розвиток сучасних методів навчання багаторівневих нейронних мереж
разом з французьким ученим ле Куном великий внесок вніс Горбань,
що сформулював і обґрунтував ``принцип подвійності'', що дозволяє
організувати економні обчислення векторів градієнта складних
функцій.

Важливий клас штучних нейронних мереж був введений фінським ученим
Кохоненом в 1987 році. В теорії мереж Кохонена використовується
алгоритмічна теорія адаптивних систем, в основному розвинена в
працях Ципкіна.

Інший напрямок дослідження штучного інтелекту розпочав Заде в 1963
році. Він запропонував концепцію нечіткої логіки. Однак, оскільки
нечітка логіка сильно потерпає від прокляття вимірності, вона
знайшла своє застосування лише для обмеженого кола систем з
невеликою кількістю вхідних змінних (функціональність камер,
пральних машин). Основу теорії нечіткого контролю вперше
запропонував Разерфорд в 1979 році. В той же час Мамдані та його
учні запропонували адаптивний нечіткий контроллер прямої дії.

В кінці 80-их вперше були помічені і використані очевидні
аналітичні властивості нейронних мереж з асоціативною пам'яттю,
коли почали застосовувати різноманітні базисні функції для
побудови узагальнених адаптивних мереж. Зокрема Брумхед
запропонував RBFN (radial basis function networks), а Харріс
B-сплайн мережу. Ці роботи проклали перший зв'язок між нейронними
мережами і нечіткою логікою, що призвело до появи нейронечітких
адаптивних мереж, які мають прозору лінгвістичну інтерпретацію від
нечіткої логіки і, водночас, аналітичне тлумачення від нейронних
мереж. Виникнення цього напрямку відображено на
рис.~\ref{fig:history}.
\begin{figure}
\centerline{\psfig{figure=CHAPTER1/EPS/History.eps,height=13cm}}
\caption{Історичний розвиток адаптивного моделювання.}
\label{fig:history}
\end{figure}
Основною проблемою нейронечітких мереж є прокляття вимірності, тож
подальші дослідження в цьому напрямі пов'язані саме з вирішенням
цієї проблеми. Найбільш значущою роботою є розробка Кавлі під
назвою ASMOD (adaptive spline modelling algorithm). Значною мірою
дана дисертаційна робота також досліджує проблему прокляття
вимірності.

В 1995 році Вапник запропонував нову теорію механізмів опорних
векторів в рамках статистичної теорії навчання. Незабаром була
розвинена теорія регресії опорних векторів.

Додати тут softcomputing.
