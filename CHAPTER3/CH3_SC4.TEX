
\section{Висновки до розділу}

Метод опорних векторів успішно застосовується для задач побудови
нелінійної регресії. Головними перевагами регресії опорних векторів
(РОВ) є її робастність, незалежність від розмірності вхідного
простору, розв'язання задачі квадратичного програмування замість
обернення погано обумовлених матриць в МНК і здатність робити
прогноз на коротких вибірках даних. Актуальним в РОВ залишається
питання автоматичного вибору гіперпараметрів. Ефективним методом
визначення оптимальних значень гіперпараметрів є байєсівський
висновок. Проте недиференційовність $\epsilon$-нечутливої функції
втрат Вапника ускладнює байєсівське підтвердження гіперпараметрів.

Запроваджена автором локально згладжена $\epsilon$-нечутлива функція
втрат забезпечує існування гессіану байєсівської моделі регресії
опорних векторів при підтвердженні гіперпараметрів. Метод Лапласа
оцінки підтвердження з локально згладженою функцією втрат генерує
критерій байєсівського підтвердження для оцінки адекватності моделі
РОВ. Для визначення гіперпараметрів байєсівської моделі РОВ доцільно
використовувати методи нелінійної оптимізації.

Розроблений автором індуктивний метод побудови байєсівської регресії
опорних векторів в характеристичному просторі поліноміальних функцій
Без'є-Бернштейна індукує збалансовані нейронечіткі моделі в формі
Бернштейна. Ці моделі легко можна інтерпретувати за допомогою
нечітких правил. При цьому за рахунок використання поліномів у формі
Бернштейна однієї та двох змінних алгоритмічна складність нечіткої
бази знань (виражена в кількості правил) є квадратичною, а не
експонентною як в нейронечітких мережах на основі B-сплайнів. Існує
можливість оцінки довірчих інтервалів. Метод також дозволяє задавати
апріорну інформацію про структуру моделі.
