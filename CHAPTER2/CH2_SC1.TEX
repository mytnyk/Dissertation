
\section{Нечітка логіка в нейронних моделях}\label{FuzzyLogicInNeuralModels}

Оскільки нечітка логіка узагальнює поняття характеристичної функції
булевої логіки, то за допомогою лінгвістичного терму $A$, що
визначається нечіткою множиною з функцією належності
$\mu_{A}(x)\in[0;1]$, можна представити нечітке твердження щодо $x$.
Класична нечітка логіка оперує з логічними функціями {\em і\/}, {\em
або\/}, {\em якщо\ldots тоді\/} над нечіткими
змінними~\cite{GlibovecOletsky02}. Приклад застосування нечіткого
логічного висновку можна знайти наприклад в
роботі~\cite{Korshevnuk-Bidyuk}. Але якщо замість звичайних
операторів $\min(\cdot)$ та $\max(\cdot)$ застосовувати алгебраїчні
оператори добутку та суми, тоді нечіткі моделі стають більш
гнучкими, і забезпечують відповідність між нейронними мережами і
нечіткою логікою, наприклад, якщо функції належності задаються у
вигляді радіальних базисних функцій або
В-сплайнів~\cite{Harris-Hong-Gan, Jang-Sun}. Такі моделі також
полегшують формальне сприйняття нелінійних систем. Дамо формальне
визначення нейронечітких моделей.
\begin{defi}\label{DefinitionOfNeuroFuzzyModel}
Нейронечіткою моделлю в канонічній формі будемо називати лінійну
комбінацію
\begin{equation}\label{gener_neuro}
f(\mathbf{x})=\sum_{i}w_i\mu_{A^i}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X}\subset
{\mathbb R^n},\quad w_i\in{\mathbb R},
\end{equation}
де $\mu_{A^i}(\mathbf{x})$ --- дійсні невід'ємні функції належності
змінної $\mathbf{x}$, які задовольняють умові
$\sum_{i}\mu_{A^i}(\mathbf{x})=1$; $A^i$ --- лінгвістичні терми
визначені на $\mathcal{X}$.
\end{defi}
Для нейронечітких моделей важливий наступний
результат~\cite{Brown-Harris, Castellano}:%(Giovana Castellano, p68-70).
\begin{thm}
\label{main_neurofuzzy_equiv} Якщо для представлення входу
$\mathbf{x}\in\mathcal{X}\subset{\mathbb R^n}$ та виходу
$y\in\mathcal{Y}\subset{\mathbb R}$ нечіткої системи
використовуються відповідно дійсні невід'ємні функції належності
$\mu_{A^i}(\mathbf{x})$ та $\mu_{B^j}(y)$, де $A^i$, $B^j$
--- лінгвістичні терми визначені відповідно на $\mathcal{X}$ та
$\mathcal{Y}$, при цьому виконуються умови
$\sum_{i}\mu_{A^i}(\mathbf{x})=1$, а також $\forall
j:\int\mu_{B^j}(y)dy=const$; якщо функції нечіткої логіки визначені
алгебраїчними операторами добутку та суми, тоді результат нечіткого
логічного висновку Мамдані, дефазифікований методом середнього
значення (центроїдним методом), відповідає виходу нейронечіткої
моделі~(\ref{gener_neuro}):
\begin{equation}
y(\mathbf{x})=\sum_{i}w_i\mu_{A^i}(\mathbf{x}).
\end{equation}
При цьому вагові коефіцієнти $w_i=\sum_{j}c_{ij}y_j^c$, де $y_j^c$
--- центр функції належності виходу $\mu_{B^j}$, $c_{ij}$ ---
ймовірність правила типу Мамдані: ``якщо $\mathbf{x}\in A^i$, тоді
$y\in B^j$'', при цьому $\sum_{j}c_{ij}=1$, $c_{ij}\in[0;1]$.
\end{thm}
Доведення цієї теореми приведено в
додатку~\ref{Neurofuzzy_Equivalence}. Задача генерації нечітких
правил на основі аналітичної нейронечіткої моделі зводиться до
відшукання ймовірностей $c_{ij}$ за заданими коефіцієнтами $w_i$.
Для цього необхідно розв'язати систему лінійних рівнянь. Розв'язок
легко отримати, якщо функції належності виходу наприклад задані у
формі В-сплайнів (1-го або 2-го порядку). В цьому випадку максимум
два коефіцієнти $c_{ij}$ для заданого $i$ ненульові.

В загальному випадку функції належності визначають не для всього
вектору входу $\mathbf{x}=(x^1,\ldots,x^n)^\top $, а для кожної
вхідної змінної $x^k$ окремо, оскільки змінні можуть мати різний
фізичний зміст і класифікуватися по-різному. Нехай область
визначення змінної $x^k$ покривається скінченою кількістю нечітких
множин $A_k^i,\;i=1,\ldots,m_k$ з функціями належності
$\mu_{A_k^i}(x^k)$:
$$
x^k\in \mathcal{X}_k \subseteq \bigcup_{i=1}^{m_k}\supp
\mu_{A_k^i}(x^k),
$$
де $\supp f$ --- компактний носій функції $f$, тобто найменша
замкнута область, зовні якої $f$ тотожно рівна $0$. А область
визначення змінної виходу $y$ покривається скінченою кількістю
нечітких множин $B^i,\;i=1,\ldots,m_0$ з функціями належності
$\mu_{B^i}(y)$:
$$
y\in\mathcal{Y} \subseteq \bigcup_{i=1}^{m_0}\supp \mu_{B^i}(y),
$$
Таким чином повний набір нечітких правил, кожне з яких можна описати
як
$$
\begin{array}{l}
r\mbox{-те правило} :\\~
\end{array}
\begin{array}{l}
\mbox{\em якщо }x^1\in A_1^{i_1}\mbox{ \em і
якщо }x^2\in A_2^{i_2}\mbox{ \em і }\ldots\\%
 \mbox{\em якщо }x^n\in A_n^{i_n}%
 \mbox{ \em тоді }y\in B^{i_0},%
\end{array}
$$
складатиметься з $R=m_0m_1\cdots m_n$ правил (індексу $r$ відповідає
послідовність $\{i_0,i_1,\ldots,i_n\}$, $i_k=0,\ldots,m_k$).
Традиційним застосуванням такого підходу є побудова нейронечіткої
моделі на основі B-сплайнів, де для формування функцій належності
для вектору входу використовується тензорний добуток B-сплайнів
однієї змінної~\cite{Harris-Hong-Gan}:
$$
\mu_{A^j}(\mathbf{x})=\prod_{k=1}^nN_{i_k}^d(x^k),\quad
j=1,\ldots,M=m_1m_2\cdots m_n,
$$
де індексу $j$ відповідає послідовність $\{i_1,\ldots,i_n\}$.
$N_i^d$ --- $i$-й B-сплайн порядку $d$, який рекурсивно
визначається на основі заданої множини вузлів $\{a_i:i=
1,\ldots,m\}$ наступним чином~\cite{Hartmut-Wolfgang}:
\begin{equation}
\label{b-splines}
\begin{array}{l}
N_i^0(x)=\left\{\begin{array}{l} 1,\quad \mbox{якщо } x\in[a_i;a_{i+1})\\
0,\quad \mbox{в іншому випадку}
\end{array}\right.,\\
N_i^d(x) = \lambda_i(x)N_i^{d-1}(x) + (1 -
\lambda_{i+1}(x))N_{i+1}^{d-1}(x),\quad
\lambda_i(x)=\frac{x-a_i}{a_{i+d}-a_i}.
\end{array}
\end{equation}

Теорема~\ref{main_neurofuzzy_equiv} показує, що нелінійна система за
допомогою нейронечіткої моделі може бути представлена у вигляді
лінійної в параметрах структури, що дозволяє застосовувати швидкі та
прості процедури навчання. Основною проблемою при цьому є прокляття
вимірності, пов'язане із експонентною складністю алгоритму навчання
$\mathcal{O}(m^n)$, що зумовлено відповідною кількістю нечітких
правил. Подальші дослідження в цьому підрозділі присвячені саме
питанням зниження складності алгоритмів навчання.
