
\section{Оптимальне навчання нейронечіткої моделі у формі Бернштейна}

\subsection{Регуляризація поліномів у формі Бернштейна}
Зауважимо, що задачі~(\ref{univar_expan}) і ~(\ref{bivar_expan})
можна інтерпретувати як задачі наближення поліномів у формі
Бернштейна однієї та двох змінних відповідно кривими та поверхнями
Без'є~\cite{Mytnyk-Bidyuk}. Надалі, для простоти викладення, не
порушуючи загальності, будемо розглядати задачу наближення поліномів
у формі Бернштейна однієї змінної кривою Без'є, тобто
задачу~(\ref{univar_expan}) у формі:
$$
y(x)=f(x)+e(x),\quad
f(x)=\sum_{j=0}^dw_j\phi_j^d\Bigl(u(x|\Lambda)\Bigr).
$$
Залишається відкритим питання щодо визначення системи базових
контрольних точок (СБКТ) $\Lambda^\ast$, яка б дозволила найкращим
чином побудувати наближення поліноміальної функції однієї змінної
кривою Без'є, тобто задовольняла певним критеріям оптимальності. Тож
під регуляризацією оберненого відображення Кастельжо (або
регуляризацією поліномів у формі Бернштейна) будемо розуміти
визначення саме такої оптимальної СБКТ, яка забезпечує найкраще
наближення поліномів у формі Бернштейна кривою Без'є за певним
критерієм. Позначимо (апостеріорну) коваріаційну матрицю оцінок
вагових коефіцієнтів (надалі в цьому підрозділі просто коваріаційну
матрицю) як $\mathbf{\Omega}_p=\cov\hat{\mathbf{w}}$, де
$\mathbf{\mathbf{w}}=\left(w_0,\ldots,w_d\right)^\top $
--- вектор вагових коефіцієнтів. Введемо наступні означення:%
\begin{defi}
\label{A-opt_def} A-оптимальною СБКТ будемо називати таку, що
мінімізує слід коваріаційної матриці:
\begin{equation}
\label{A-opt_def_eq}
\Lambda_{\mbox{\small{A-opt}}}^{\ast}:\tr\mathbf{\Omega}_p\longrightarrow\min.
\end{equation}
\end{defi}
Мінімізація сліду коваріаційної матриці відповідає мінімізації
середньої дисперсії оцінок коефіцієнтів, що має зміст суми квадратів
головних півосей еліпсоїду розсіювання оцінок~\cite{Krug-Sosulin,
Malutov-Zaigraev}.
\begin{defi}
\label{D-opt_def} D-оптимальною СБКТ будемо називати таку, що
мінімізує значення визначника відповідної коваріаційної матриці:
\begin{equation}
\Lambda_{\mbox{\small{D-opt}}}^{\ast}:\det\mathbf{\Omega}_p\longrightarrow\min,
\end{equation}
\end{defi}
Такий визначник має зміст об'єму еліпсоїду розсіювання оцінок,
тобто узагальненої дисперсії оцінок невідомих вагових коефіцієнтів
рівняння регресії.
\begin{defi}
\label{G-opt_def} G-оптимальною СБКТ будемо називати таку, що
мінімізує максимальну дисперсію спрогнозованих значень регресії:
\begin{equation}
\Lambda_{\mbox{\small{G-opt}}}^{\ast}:\max_{x\in
\mathcal{X}}\left\{\varl{f(x)}\right\}\longrightarrow\min,
\end{equation}
де дисперсія спрогнозованих значень визначається як
$$
\varl{f(x)}=\mathbf{a}^\top(x)\mathbf{\Omega}_p\mathbf{a}(x), \quad
\mathbf{a}(x)=\left(\phi_0^d\left(u(x|\Lambda)\right),\ldots,\phi_d^d\left(u(x|\Lambda)\right)\right)^\top,
$$
де $u(x|\Lambda)$ --- барицентричні координати визначені за
допомогою оберненого відображення Кастельжо на основі СБКТ $\Lambda$
для $x\in\mathcal{X}\subset {\mathbb R}$.
\end{defi}
\begin{defi}
I-оптимальною СБКТ будемо називати таку, що мінімізує узагальнену
дисперсію спрогнозованих значень регресії~\cite{Hardin-Sloane}:
\begin{equation}
\label{I-opt_def}
\Lambda_{\mbox{\small{I-opt}}}^{\ast}:J=\int_\mathcal{X}
\varl{f(x)}d\mu(x)\longrightarrow\min,
\end{equation}
де $\mu$ деяка міра на просторі входу $\mathcal{X}$.
\end{defi}
Припускаючи, що випадкова складова моделі~(\ref{gabor_expan}) має
нормальний розподіл $e(x)\sim \mathcal{N}(0, \sigma^2)$, а для
отримання оцінок невідомих коефіцієнтів застосовується МНК, тоді
коваріаційна матриця матиме вигляд:
\begin{multline*}
\mathbf{\Omega}_p=\cov\hat{\mathbf{w}}=
\meanl{(\hat{\mathbf{w}}-\mathbf{w})(\hat{\mathbf{w}}-\mathbf{w})^\top}=\\
=\mean{\left(\left(\mathbf{A}^\top\mathbf{A}\right)^{-1}\mathbf{A}^\top\mathbf{y}-
\left(\mathbf{A}^\top\mathbf{A}\right)^{-1}\mathbf{A}^\top\mathbf{f}\right)
\left(\left(\mathbf{A}^\top\mathbf{A}\right)^{-1}\mathbf{A}^\top\mathbf{y}-
\left(\mathbf{A}^\top\mathbf{A}\right)^{-1}\mathbf{A}^\top\mathbf{f}\right)^\top}\\
=\left(\mathbf{A}^\top\mathbf{A}\right)^{-1}\mathbf{A}^\top
\meanl{(\mathbf{y}-\mathbf{f})(\mathbf{y}-\mathbf{f})^\top}\mathbf{A}\left(\mathbf{A}^\top\mathbf{A}\right)^{-1}
=\sigma^2\left(\mathbf{A}^\top\mathbf{A}\right)^{-1},
\end{multline*}
де матриця
$\mathbf{A}=(\mathbf{a}_1,\ldots,\mathbf{a}_N)^\top=\left\{\phi_j^d\left(u(x_i|\Lambda)\right)\right\}_{ij}$,
$j=0,\ldots,d$; $i=1,\ldots,N$,
$\mathbf{y}=(\mathbf{y}_1,\ldots,\mathbf{y}_N)^\top$,
$\mathbf{f}=(\mathbf{f}(x_1),\ldots,\mathbf{f}(x_N))^\top$, $N$ ---
об'єм навчальної вибірки. Надалі будемо вважати $\sigma^2=1$, що не
обмежує загальності задач~(\ref{A-opt_def_eq}-\ref{I-opt_def}).

Застосування теорії побудови оптимальних планів в нашому випадку
обмежується пасивною ідентифікацією, оскільки розглядаються системи,
в яких неможливо ставити експеримент і задавати значення входів,
наприклад, економічні системи. Тож мова йде про оптимальний розподіл
параметрів моделі, де в ролі параметрів виступають базові контрольні
точки. Як правило A-, D-оптимальність використовується для побудови
моделей, для яких важлива точність і якість оцінок коефіцієнтів
регресії. В роботі~\cite{Chen-Hong-Harris} D-оптимальність
використовується для побудови робастних моделей нелінійних систем за
допомогою локально регуляризованого методу найменших квадратів. В
той час як G-, I-оптимальність ефективна для задач пошуку екстремуму
поверхні відгуку в заданій області. Важливо визначити, які з
наведених оптимізаційних задач ефективніші для визначення структури
моделі, яка забезпечує найменшу похибку при прогнозуванні.

\subsection{Застосування еволюційних методів для визначення оптимального
розподілу базових контрольних точок}

Відомі приклади успішного застосування генетичних алгоритмів (ГА) в
задачах оптимального планування експерименту~\cite{Poland-Mitterer},
для ідентифікації нелінійних залежностей~\cite{Paklin}, а також для
деяких оптимізаційних задач~\cite{Bidyuk-Mytnyk,
Bidyuk-Baklan-Mytnyk-Lytvinenko-conf,Bidyuk-Lytvinenko-Mytnyk}. Тож
для розв'язку оптимізаційних
задач~(\ref{A-opt_def_eq}-\ref{I-opt_def}) пропонується
використовувати ГА, а саме його реалізацію з популяціями, які не
перекриваються~\cite{Rogers-Prugel}. Зазначимо, що в цьому випадку
задачі~(\ref{univar_expan}) і~(\ref{bivar_expan}) можна віднести до
класу задач навчання нейронних систем еволюційними методами.

Об'єкти ГА в нашому випадку мають наступний зміст. В якості {\em
гену} виступає базова контрольна точка. Набір {\em алелей}
характеризує множину можливих значень контрольних точок. Кожна
{\em особина} ({\em хромосома}) складається з набору генів, який
відповідає системі контрольних точок. {\em Мірою придатності}
кожної особини, яка виражає той чи інший критерій оптимальності,
виступає цільова функція вигляду~\cite{Mytnyk-6conf}:
$$
F_{\mbox{\small{A-opt}}}(\Lambda)\sim-\tr\mathbf{\Omega}_p,\quad
F_{\mbox{\small{D-opt}}}(\Lambda)\sim-\det\mathbf{\Omega}_p.
$$

В результаті роботи ГА на тестових моделях були отримані оптимальні
розподіли базових контрольних точок для різних розподілів вектору
входу~\cite{Mytnyk-ISDMIT-2005, Mytnyk-DSMSI-2005}. При цьому
точність апроксимації і короткострокового прогнозування виходу
системи за допомогою моделі К.Харріса зросла в декілька разів,
оскільки наведені вище оптимізації зменшують також і значення
середньо-квадратичної помилки. Хоч швидкість роботи наведеного ГА
відносно невелика, вона може бути підвищена за рахунок використання
паралельної обробки інформації~\cite{Bidyuk-Litvinenko}. Окремою
темою для досліджень в цьому напрямку є питання про збіжність ГА.
Хоч ГА іноді знаходить гарні розв'язки для складних задач, проте
теоретичних результатів щодо збіжних властивостей і умов збіжності
дуже мало. Одна із спроб дослідити ГА на збіжність
--- створення відповідної моделі ланцюга Маркова і ймовірнісний
аналіз збіжності найпридатнішої особини до глобального
екстремуму~\cite{Eiben-Aarts}. Для задач побудови довгострокового
прогнозу проведення таких оптимізацій як A, D, I, G недостатньо,
оскільки середньо-квадратична помилка, яка використовується при
побудові регресії на всіх точках вибірки, є внутрішнім критерієм
системи. Для однозначного вибору моделі (параметрів моделі) як
правило використовується принцип зовнішнього доповнення і відповідно
зовнішні критерії~\cite{Ivahnenko}. В нашому випадку в якості
зовнішнього доповнення використаємо додаткову перевірочну вибірку
для наступного критерію оптимальності.
\begin{defi}
Оптимальною за виходом (О-оптимальною) СБКТ будемо називати таку, що
мінімізує функцію ризику:
\begin{equation}
\label{O-opt_def}
\Lambda_{\mbox{\small{O-opt}}}^{\ast}:R_{p}(\cdot)\longrightarrow\min,
\end{equation}
де функція ризику може бути визначена в різний спосіб:
$$
R_{p}(\cdot)=\langle \mean{|\delta(x)|}, \mean{|\delta(x)|^2},
\max_{x}|\delta(x)| \rangle, \quad x\in \mathcal{D}_{N_p}\subset
\mathcal{D},
$$
де $\delta(x)=y(x)-f(x)$ є похибкою наближення поліному у формі
Бернштейна однієї змінної кривою Без'є. $\mathcal{D}_{N_p}$
--- перевірочна вибірка даних.
\end{defi}
При $R_{p}(\cdot)=\mean{|\delta(x)|^2}$ оптимізаційна
задача~(\ref{O-opt_def}) виражає відомий середньо-квадратичний
критерій регулярності~(\ref{RiskCrossValid}). Цільова функція ГА для
знаходження СБКТ оптимальної за виходом має вигляд:
$$
F_{\mbox{\small{O-opt}}}(\Lambda)\sim-R_{p}(\cdot).
$$
Мінімальну похибку прогнозу моделі К.Харріса було отримано саме із
застосуванням оптимальної за виходом СБКТ, яка забезпечувала
оптимальне нечітке представлення входу~\cite{Mytnyk-BBR-2006}.


\subsection{Деякі оцінки оптимальностей}

\subsubsection*{Оцінка I-оптимальності}
Функціонал~(\ref{I-opt_def}) легко спрощується до вигляду
$J=\tr\left(\mathbf{M}\mathbf{\Omega}_p\right)$, де матриця моментів
вхідного простору
$\mathbf{M}=\int_\mathcal{X}\mathbf{a}(x)\mathbf{a}^\top(x)d\mu(x)$.
Покажемо це:
$$
J=\tr(\mathbf{J}\equiv J)=\int_\mathcal{X}\tr\left(
\mathbf{a}^\top(x)\mathbf{\Omega}_p\mathbf{a}(x)\right)d\mu(x)=
$$
$$
\int_\mathcal{X}\tr\left(\mathbf{a}(x)
\mathbf{a}^\top(x)\mathbf{\Omega}_p\right)d\mu(x)=
\tr\left(\int_\mathcal{X}\mathbf{a}(x)
\mathbf{a}^\top(x)d\mu(x)\cdot\mathbf{\Omega}_p\right),
$$
оскільки для будь-яких квадратних матриць $\mathbf{A}$ і
$\mathbf{B}$ виконується рівність
$\tr(\mathbf{AB})=\tr(\mathbf{BA})$. Цей результат полегшує
обчислення показника I-опти\-маль\-ності, оскільки в багатьох
випадках матрицю моментів можна визначити наперед. Крім того,
очевидно, що у випадку ортогональності векторів $\mathbf{a}(x)$,
матриця моментів є одиничною матрицею і I-оптимальність зводиться до
A-оптимальності.

\subsubsection*{Оцінка базисних поліномів Бернштейна}
\begin{sta}
\label{Bernstein_poly_estim_trm} Будь-який базисний поліном
Бернштейна однієї змінної порядку $d$ допускає оцінку зверху:
\begin{equation}
\label{Bern_top_estimation}
\forall u\in(0;1),\:\forall j : \phi_j^d(u)\lesssim%\leqslant
\rho(u)=\frac{1}{\sqrt{2\pi du(1-u)}},
\end{equation}
при чому $\phi_j^d(u)\approx \rho(u)$ при $u=j/d$
(рис.~\ref{fig:bern_estim}).
\end{sta}
\begin{figure}[ht]
\safepdf{\centerline{\includegraphics[height=10cm]{CHAPTER2/EPS/bern_estim.pdf}}}
{\centerline{\psfig{figure=CHAPTER2/EPS/bern_estim.eps,height=10cm}}}
\caption{Оцінка базисних поліномів Бернштейна 4-го порядку зверху.}
\label{fig:bern_estim}
\end{figure}
\begin{proof}
Базисні поліноми Бернштейна однієї змінної порядку $d$ мають вигляд:
$$
\phi_j^d(u)=\dbinom{d}{j} \cdot u^j(1-u)^{d-j},
$$
і до них, як і для всяких біноміальних розподілів, можна
застосувати нормальне наближення. Для довільного симетричного
біноміального розподілу справедливі наступні оцінки його членів,
які перенумеровані в залежності від їх відстані до максимальної
ймовірності~\cite{Feller}:
$$
\begin{array}{l}
a_k=\dbinom{d}{m+k}\cdot u^{m+k}(1-u)^{d-m-k}\sim hr(hk),\\
k=-m,\ldots,0,\ldots,d-m,\quad j=m+k,
\end{array}
$$
де $m=du+\delta$, $u-1<\delta\leqslant u$ --- індекс максимальної
ймовірності розподілу, $h=\frac{1}{\sqrt{du(1-u)}}$,
$r(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$
--- щільність нормального розподілу. Точність такого
наближення детально розглядається в роботі~\cite{Feller}, де %стор. 198
наведена оцінка логарифмічної похибки наближення $\varsigma_k$:
$$
|\varsigma_k|<\frac{k^3}{(du(1-u))^2}+\frac{2k}{du(1-u)}.
$$
Очевидно що
$$
\forall u\in(0;1),\:\forall j : \phi_j^d(u)=a_k\leqslant a_0\sim
hr(h\cdot
0)=\frac{1}{\sqrt{du(1-u)}}\cdot\frac{1}{\sqrt{2\pi}}=\rho(u),
$$
що доводить оцінку зверху для базисних поліномів
Бернштейна~(\ref{Bern_top_estimation}). Крім того, оцінка
максимальної ймовірності $a_0\sim \rho(u)$ є одночасно і оцінкою
максимуму кожного базисного поліному Бернштейна при
$u\longrightarrow j/d$:
$$
\forall u\in [0;1],\:\forall j : \phi_j^d(u)\leqslant
\phi_j^d\left(u_j=\frac{j}{d}\right)\sim\rho(u_j)=\frac{1}{\sqrt{2\pi
du_j(1-u_j)}},
$$
при чому $\forall k : \phi_j^d(u_j)\geqslant \phi_k^d(u_j)$.
\end{proof}

\subsubsection*{Оцінка сліду інформаційної матриці Фішера}

\begin{sta}
\label{Inform_matrix_estim_trm} Слід інформаційної матриці Фішера
від системи базисних поліномів Бернштейна допускає наближення
(рис.~\ref{fig:trace_inf_estim}):
\begin{equation}
\label{Inform_matrix_estim}%
\tr(\mathbf{A}^\top
\mathbf{A})\approx\sum_{i=1}^N\frac{1}{2\sqrt{\pi du_i(1-u_i)}}.
\end{equation}
\end{sta}
\begin{figure}[ht]
\safepdf{\centerline{\includegraphics[height=10cm]{CHAPTER2/EPS/trace_inf_estim.pdf}}}
{\centerline{\psfig{figure=CHAPTER2/EPS/trace_inf_estim.eps,height=10cm}}}
\caption{Оцінка сліду інформаційної матриці Фішера.}
\label{fig:trace_inf_estim}
\end{figure}
\begin{proof}
Враховуючи вище наведені оцінки для базисних поліномів Бернштейна,
справедливо наступне:
$$
\sum_{j=0}^d\bigl(\phi_j^d(u_i)\bigr)^2\sim%
\sum_{k=-m}^{d-m}\bigl(hr(kh)\bigr)^2\stackrel{\langle
k\rightarrow \xi\rangle}{\approx}%
\frac{1}{2\pi}h^2\int_{-\infty}^{+\infty}e^{-\xi^2h^2}d\xi
\stackrel{\langle x=\sqrt{2}\xi h\rangle}{=}
$$
$$
=\frac{1}{2\pi}\frac{h}{\sqrt{2}}\int_{-\infty}^{+\infty}e^{-x^2/2}dx=
\frac{h}{2\sqrt{\pi}}=\frac{1}{2\sqrt{\pi du_i(1-u_i)}}.
$$
Відповідно для сліду інформаційної матриці Фішера (надалі просто
інформаційної матриці) отримаємо:
$$
\tr(\mathbf{A}^\top
\mathbf{A})=\sum_{j=0}^d\sum_{i=1}^N\bigl(\phi_j^d\left(u(x_i|\Lambda)
\right)\bigr)^2\approx\sum_{i=1}^N\frac{1}{2\sqrt{\pi du_i(1-u_i)}}.
$$
\end{proof}


З твердження~\ref{Inform_matrix_estim_trm} можна зробити деякі
висновки:
\begin{list}{$\triangleright$}{}
\item Очевидно, що чим ближчі барицентричні координати до центру
(до $0.5$), тим менше значення сліду інформаційної матриці. %
\item При рівномірному розподілі барицентричної координати на
$[0;1]$ можна перейти від граничних сум до визначеного інтегралу,
який
легко обчислити через тригонометричну заміну: %
\begin{equation}
\label{Tr_Uniform_baric} \tr(\mathbf{A}^\top \mathbf{A})\sim
\frac{N}{2\sqrt{\pi
d}}\int_{0}^{1}\frac{du}{\sqrt{u(1-u)}}=\frac{N}{2\sqrt{\pi
d}}\cdot\pi=\frac{N}{2}\sqrt{\frac{\pi}{d}}.
\end{equation}
\end{list}

\subsubsection*{Оцінка A,D-оптимальності на основі власних чисел}

Зазначимо, що коваріаційна матриця --- це симетрична, позитивно
визначена матриця, власні числа якої дійсні та невід'ємні, а їх
сума рівна сліду матриці. За допомогою нерівностей між середнім
арифметичним та середнім геометричним можна отримати оцінку знизу
для сліду коваріаційної матриці:
\begin{multline}
\label{A-opt_trace_cov}%
\tr\mathbf{\Omega}_p=\tr(\mathbf{A}^\top \mathbf{A})^{-1}=
\sum_{i=0}^d\frac{1}{\lambda_i}\geqslant(d+1)\sqrt[d+1]{\prod_{i=0}^d\frac{1}{\lambda_i}}\geqslant\\
\geqslant(d+1)^2\left[\sum_{i=0}^d\lambda_i\right]^{-1}=
(d+1)^2\left[\tr(\mathbf{A}^\top \mathbf{A})\right]^{-1},
\end{multline}
де $\lambda_i^{-1}$ --- власні числа коваріаційної матриці,
$\lambda_i$ --- власні числа інформаційної матриці. Аналогічно
визначник коваріаційної матриці, який дорівнює добутку власних
чисел, також допускає оцінку знизу:
\begin{multline}
\label{D-opt_trace_cov} \det\mathbf{\Omega}_p=\det(\mathbf{A}^\top
\mathbf{A})^{-1}=
\prod_{i=0}^d\frac{1}{\lambda_i}\geqslant(d+1)^{d+1}\left[\sum_{i=0}^d\lambda_i\right]^{-d-1}=\\
=(d+1)^{d+1}\left[\tr(\mathbf{A}^\top \mathbf{A})\right]^{-d-1}\approx%
(d+1)^{d+1}\left[\sum_{i=1}^N\frac{1}{2\sqrt{\pi
du_i(1-u_i)}}\right]^{-d-1}.
\end{multline}
Очевидно, що ті розподіли барицентричних координат, для яких в
нерівностях~(\ref{A-opt_trace_cov}, \ref{D-opt_trace_cov}) мають
місце рівності, тобто A,D-оптимальні розподіли, співпадають і
можуть визначатися умовою~\cite{Malutov-Zaigraev}:
$$
\mathbf{A}^\top
\mathbf{A}\approx(d+1)^{-1}\sum_{i=1}^N\frac{1}{2\sqrt{\pi
du_i(1-u_i)}}\mathbf{I}_{d+1}.
$$
Але легко показати, що така матриця не існує. Це слідує з того, що
базисні поліноми Бернштейна, з яких складається матриця
$\mathbf{A}$, додатні. Тож можна зробити висновок про те, що
оптимальність у нашому випадку це паритет двох критеріїв:
\begin{enumerate}
\item Кожна барицентрична координата повинна знаходитись якомога
ближче до центру області визначення. %
\item Барицентричні координати повинні розташовуватися якомога
далі одна від одної, з метою покращення обумовленості інформаційної матриці.%
\end{enumerate}
Зауважимо, що наведені вище оцінки знизу для A, D-оптимальностей
не слід використовувати як критерії мінімізації самих
оптимальностей, тобто мінімізація сліду чи визначника
коваріаційної матриці не еквівалентна максимізації сліду
інформаційної матриці. Зазначимо також, що для неперервних планів
справедливою є теорема еквівалентності~\cite{Krug-Sosulin}, згідно
якої неперервний D-оптимальний план є також і G-оптимальним.
