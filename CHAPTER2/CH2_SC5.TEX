
\section{Збалансовані нейронечіткі моделі}

Головною проблемою використання класичних нечітких баз знань є їх
експонентна складність, яка виражена у великій кількості нечітких
правил. Це викликано необхідністю створення нового правила для
кожної нової комбінації належностей факторів до нечітких множин.
Більш детально це розглянуто в
підрозділі~\ref{FuzzyLogicInNeuralModels} Це робить нечіткі бази
знань непрозорими для експертів, оскільки практично неможливо
застосовувати їх напряму для отримання корисної інформації. Більш
того, для здійснення нечіткого логічного висновку необхідно знати
значення всіх факторів, що входять до правила. Тобто неможливо
зробити висновок за відсутності знань щодо хоча б одного з факторів.

З метою підвищення прозорості і зменшення кількості нечітких правил
доцільно провести декомпозицію нечіткої бази знань. Фактично це
означає, що доцільно провести декомпозицію нейронечіткої моделі на
менші підмоделі, які описують залежність вихідного фактору від
окремого вхідного фактору або максимум пари вхідних факторів. Таким
чином складність нечіткої бази знань може бути значно зменшена --- з
експонентної до квадратичної. При цьому повинна існувати можливість
застосовувати нечіткий логічний висновок окремо для кожної
нейронечіткої підмоделі та комбінувати результати цих висновків.
Саме цього намагаються досягти узагальнені нейронечіткі моделі
К.Харріса (нейронечіткі моделі у формі Бернштейна), які
використовують поліноми у формі Бернштейна однієї та двох змінних в
якості підмоделей. Проте залишається необґрунтованим незалежне
використання таких підмоделей. В цьому підрозділі ми сформуємо
спеціальний клас так званих збалансованих нейронечітких моделей, для
яких незалежне використання підмоделей буде нами обґрунтовано. І
покажемо, яким чином нейронечіткі моделі у формі Бернштейна можуть
бути зведені до збалансованих.
\begin{defi}
Нейронечіткі моделі вигляду
$$
f(\mathbf{x})=b+
\sum_{k=1}^ng_k(x^k)+\sum_{p=1}^{n-1}\sum_{q=p+1}^ng_{pq}\left(x^p,
x^q\right),
$$
де
$$
g_k(x^k)=\sum_iw_i^k\mu_{A^i_k}(x^k),\quad
g_{pq}(x^p,x^q)=\sum_jw_j^{pq}\mu_{A^j_{pq}}(x^p,x^q),
$$
будемо називати збалансованими, якщо математичне сподівання функцій
$g_k$, $g_{pq}$ за умови багатовимірного рівномірного розподілу
випадкової величини $\mathbf{x}$ на своїй області визначення
дорівнює нулю. При цьому нейронечіткі підмоделі визначаються як
$f_k=b+g_k$ та $f_{pq}=b+g_{pq}$. Величина $b$ називається зміщенням
нейронечіткої збалансованої моделі і характеризує середнє значення
виходу.
\end{defi}
Таким чином, за відсутності знань щодо значення змінної $x^k$
логічно припустити, що змінна $x^k$ з однаковою ймовірністю може
приймати будь-яке значення зі своєї області визначення. При цьому
вплив цієї змінної на вихід моделі після розкриття невизначеності за
допомогою математичного сподівання буде нульовим. Таким чином
збалансованість дозволяє генерувати нечіткі правила на основі
окремої підмоделі незалежно від інших підмоделей. Означення
збалансованості коректне і не суперечить
означенню~\ref{DefinitionOfNeuroFuzzyModel}, оскільки нейронечіткі
підмоделі можуть бути легко приведені до канонічної форми:
$$
f_k(x^k)=b+\sum_iw_i^k\mu_{A^i_k}(x^k)=\sum_i(w_i^k+b)\mu_{A^i_k}(x^k),
$$
$$
f_{pq}(x^p,x^q)=b+\sum_jw_j^{pq}\mu_{A^j_{pq}}(x^p,x^q)=\sum_j(w_j^{pq}+b)\mu_{A^j_{pq}}(x^p,x^q),
$$
в силу властивостей $\sum_{i}\mu_{A^i_k}(x^k)=1$,
$\sum_j\mu_{A^j_{pq}}(x^p,x^q)=1$.

\begin{lem}[достатня умова збалансованості нейронечіткої
моделі]\label{SufficientBalancingCondition} Нехай випадкова величина
$\mathbf{x}$ рівномірно розподілена на своїй області визначення.
Тоді якщо
\begin{equation}\label{FirstBalancingCondition}
\forall i: \meanl{\mu_{A^i_k}(x^k)}=const_k,\quad\forall j:
\meanl{\mu_{A^j_{pq}}(x^p,x^q)}=const_{pq},
\end{equation}
а також сума коефіцієнтів при функціях належності рівна нулю:
$$
\sum_iw_i^k=0,\quad\sum_jw_j^{pq}=0,
$$
тоді відповідна нейронечітка модель є збалансованою.
\end{lem}
\begin{proof}[Доведення леми про достатню умову збалансованості]
Як легко бачити, математичне сподівання функцій $g_k(x^k)$,
$g_{pq}(x^p,x^q)$ рівне нулю:
$$
\meanl{g_k(x^k)}=\sum_iw_i^k\meanl{\mu_{A^i_k}(x^k)}=0\cdot
const_k=0,
$$
$$
\meanl{g_{pq}(x^p,x^q)}=\sum_jw_j^{pq}\meanl{\mu_{A^j_{pq}}(x^p,x^q)}=0\cdot
const_{pq}=0.
$$
\end{proof}

\begin{thm}\label{CastingToBalancedModel}
Якщо виконується умова~(\ref{FirstBalancingCondition}), але сума
коефіцієнтів при функціях належності не рівна нулю:
$$
\sum_iw_i^k=\Lambda_k\neq0,\quad\sum_jw_j^{pq}=\Lambda_{pq}\neq0,
$$
то нейронечітку модель можна звести до збалансованої.
\end{thm}
\begin{proof}
Для кожної підмоделі виконаємо наступні перетворення:
$$
g_k(x^k)=\sum_{i=1}^{L_k}w_i^k\mu_{A^i_k}(x^k)=\sum_{i=1}^{L_k}\left(w_i^k-\frac{\Lambda_k}{L_k}\right)\mu_{A^i_k}(x^k)
+\frac{\Lambda_k}{L_k},
$$
$$
g_{pq}(x^p,x^q)=\sum_{j=1}^{L_{pq}}w_j^{pq}\mu_{A^j_{pq}}(x^p,x^q)=
\sum_{j=1}^{L_{pq}}\left(w_j^{pq}-\frac{\Lambda_{pq}}{L_{pq}}\right)\mu_{A^j_{pq}}(x^p,x^q)
+\frac{\Lambda_{pq}}{L_{pq}}.
$$
Перевизначимо зміщення та коефіцієнти при функціях належності
$$
\tilde{w}_i^k\longleftarrow w_i^k-\frac{\Lambda_k}{L_k},\quad
\tilde{w}_j^{pq}\longleftarrow
w_j^{pq}-\frac{\Lambda_{pq}}{L_{pq}},\quad \tilde{b}\longleftarrow
b+\sum_{k=1}^n\frac{\Lambda_k}{L_k}+\sum_{p>q}^n\frac{\Lambda_{pq}}{L_{pq}}.
$$
Легко перевірити, що новостворена нейронечітка модель вигляду
$$
\tilde{f}(\mathbf{x})=\tilde{b}+
\sum_{k=1}^n\tilde{g}_k(x^k)+\sum_{p=1}^{n-1}\sum_{q=p+1}^n\tilde{g}_{pq}\left(x^p,
x^q\right),
$$
де
$$
\tilde{g}_k(x^k)=\sum_i\tilde{w}_i^k\mu_{A^i_k}(x^k),\quad
\tilde{g}_{pq}(x^p,x^q)=\sum_j\tilde{w}_j^{pq}\mu_{A^j_{pq}}(x^p,x^q),
$$
задовольняє обом умовам леми \ref{SufficientBalancingCondition} і є
збалансованою. При цьому $\tilde{f}(\mathbf{x})=f(\mathbf{x})$.
\end{proof}

Теорему~\ref{CastingToBalancedModel} можна застосувати для
нейронечітких моделей у формі Бернштейна і отримати збалансовані
нейронечіткі моделі у формі Бернштейна, оскільки справедливі
наступні твердження, які задовольняють
умову~(\ref{FirstBalancingCondition}). При цьому обернене
відображення Кастельжо повинно бути побудоване так, щоб рівномірний
розподіл змінної $\mathbf{x}$ на своїй області визначення індукував
рівномірний розподіл відповідних барицентричних координат.
\begin{sta}\label{bernstein_poly mean_const_prop}
Математичне сподівання будь-якого базисного поліному Бернштейна
$\phi_j^d(u)$ порядку $d$ за умови рівномірного розподілу випадкової
величини $u$ на $[0;~1]$ рівне $1/(d+1)$.
\end{sta}
\begin{proof}
Доведемо, що математичне сподівання всіх базисних поліномів
Бернштейна порядку $d$ однакове. Для цього достатньо показати, що
$\meanl{\phi_j^d(u)}=\meanl{\phi_{j+1}^d(u)}$.
$$
\meanl{\phi_j^d(u)}=\int_0^1\phi_j^d(u)P(u)du=\int_0^1\dbinom{d}{j}
u^j(1-u)^{d-j}P(u)du.
$$
Оскільки випадкова величина $u$ розподілена рівномірно на $[0;~1]$,
то щільність розподілу $P(u)=1$. Обчислюючи інтеграл за частинами,
маємо
\begin{multline*}
\meanl{\phi_j^d(u)}=\int_0^1\dbinom{d}{j}u^j(1-u)^{d-j}du=
\int_0^1\dbinom{d}{j}\frac{1}{j+1}(1-u)^{d-j}du^{j+1}=\\
=\dbinom{d}{j}\frac{1}{j+1}(1-u)^{d-j}u^{j+1}\Bigl|_0^1-
\int_0^1\dbinom{d}{j}\frac{1}{j+1}u^{j+1}d(1-u)^{d-j}=\\
=0-0+\int_0^1\dbinom{d}{j}\frac{d-j}{j+1}u^{j+1}(1-u)^{d-j-1}du=\\
=\int_0^1\dbinom{d}{j+1}u^{j+1}(1-u)^{d-j-1}du=\meanl{\phi_{j+1}^d(u)},
\end{multline*}
Це також означає, що $\meanl{\phi_j^d(u)}=\meanl{\phi_d^d(u)}$.
$$
\meanl{\phi_d^d(u)}=\int_0^1\dbinom{d}{d}u^d(1-u)^{d-d}du=
\frac{u^{d+1}}{d+1}\Bigl|_0^1=\frac{1}{d+1}.
$$
Цей результат можна отримати і з інших міркувань. А саме, якщо
загальна кількість базисних поліномів рівна $d+1$, а площі під
базисними поліномами Бернштейна однакові і в сумі дорівнюють 1 (це
слідує з властивості базисних поліномів Бернштейна), то кожна площа
рівна $1/(d+1)$.
\end{proof}
Аналогічно можна довести що:
\begin{sta}
Математичне сподівання будь-якого базисного поліному Бернштейна
$\phi_{ijk}^d(\mathbf{u})$ порядку $d$ за умови рівномірного
розподілу випадкової величини $\mathbf{u}$ на
$\triangle\{u\geqslant0, v\geqslant0, u+v\leqslant1 \}$ рівне
$2/((d+1)(d+2))$.
\end{sta}


\section{Висновки до розділу}

Метод побудови нейронечітких моделей у формі Бернштейна, в якому
базисні поліноми Бернштейна використовуються в якості функцій
належності, доцільно використовувати для синтезу нечітких баз знань
квадратичної складності. Серед недоліків базового методу слід
відзначити невизначеність вибору базових контрольних точок та
негативний вплив ітеративності чисельного методу знаходження
барицентричних координат на швидкодію алгоритму. Розроблений автором
прискорений метод навчання нейронечітких моделей у формі Бернштейна,
де для визначення барицентричних координат використовується швидке
обернене відображення Кастельжо, позбавлений цих недоліків. При
цьому швидкість навчання нейронечітких моделей у формі Бернштейна на
тестових вибірках зросла в 3--4 рази. Для обґрунтування незалежного
використання нечітких правил, які сформовані на основі окремих
підмоделей, автором введені збалансовані нейронечіткі моделі. Тож
при формуванні нечітких баз знань пониженої складності слід
спиратися на збалансовані нейронечіткі моделі у формі Бернштейна.

Теорія побудови оптимальних планів дозволяє забезпечити робастність
нейронечітких моделей у формі Бернштейна. Так, А-оптимальна система
базових контрольних точок мінімізує слід матриці коваріації оцінок
вагових коефіцієнтів нейронечіткої моделі. Відповідно для визначення
барицентричних координат використовується оптимальне обернене
відображення Кастельжо. Для визначення оптимального розміщення
контрольних точок можна використовувати еволюційні методи, зокрема
генетичний алгоритм. Проте використання такого класу алгоритмів
пов'язано з значними обчислювальними витратами. І в наступному
розділі буде розглянуто інший спосіб забезпечення робастності оцінок
параметрів регресії, а саме --- регресія опорних векторів.
